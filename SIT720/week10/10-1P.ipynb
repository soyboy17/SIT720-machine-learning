{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.1P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question 1:  Create a MLP model with 10 hidden layers using  \"data.csv\" dataset and report performances using appropriate metrics**_\n",
    "\n",
    "For this question I will use 10 hidden layers and run the classifier with arbitrarily selected values of 10, 25, and 100 neurons to see if/how it affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 43876 rows with 101 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  t_8  t_9  ...  t_91  t_92  t_93  \\\n",
       "0  112  274  158  215  274  158  215  298   76  208  ...    71   297   135   \n",
       "1   82  208  187  208  172  117  172  117  172  117  ...    81   240   117   \n",
       "2   16  110  240  117  240  117  240  117  240  117  ...    65   112   123   \n",
       "3   82  208  187  208  172  117  172  117  172  117  ...   208   302   208   \n",
       "4   82  240  117  240  117  240  117  240  117  172  ...   209   260    40   \n",
       "\n",
       "   t_94  t_95  t_96  t_97  t_98  t_99  malware  \n",
       "0   171   215    35   208    56    71        1  \n",
       "1    71   297   135   171   215    35        1  \n",
       "2    65   112   123    65   113   112        1  \n",
       "3   302   187   208   302   228   302        1  \n",
       "4   209   260   141   260   141   260        1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in dataset\n",
    "df = pd.read_csv('../data/data_10.csv')\n",
    "\n",
    "# Print dimensions\n",
    "rows, cols = df.shape\n",
    "print(\"Data has {} rows with {} columns\".format(rows, cols))\n",
    "\n",
    "# Show head\n",
    "df.head()\n",
    "\n",
    "#df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t_0        0\n",
       "t_1        0\n",
       "t_2        0\n",
       "t_3        0\n",
       "t_4        0\n",
       "          ..\n",
       "t_96       0\n",
       "t_97       0\n",
       "t_98       0\n",
       "t_99       0\n",
       "malware    0\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show unique values for malware - already numeric so no need to convert\n",
    "df['malware'].unique()\n",
    "\n",
    "#Check for missing values - none found\n",
    "df.isnull().sum()\n",
    "\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35100 samples for training and 8776 samples for testing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('malware', axis=1)\n",
    "y = df['malware']\n",
    "\n",
    "feat_cols = X.columns\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "xtrain_samples = X_train.shape[0]\n",
    "xtest_samples = X_test.shape[0]\n",
    "\n",
    "print(f'There are {xtrain_samples} samples for training and {xtest_samples} samples for testing.')\n",
    "\n",
    "# Standardise data with standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MLP model with 10 hidden layers with 10 neurons each\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10,10,10,10,10,10,10,10,10), max_iter=2000, random_state=42) # max_iter default is 200\n",
    "\n",
    "# Fit the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.62      0.70       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.89      0.81      0.84      8776\n",
      "weighted avg       0.98      0.99      0.99      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 140   86]\n",
      " [  36 8514]]\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics for MLP model with 10 hidden layers with 10 neurons each\n",
    "\n",
    "# Accuracy rounded to 3 decimal places\n",
    "print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 3))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## An accuracy score of 0.986 indicates that the model classifies data very well\n",
    "## The model correctly classified 130 positives and 8523 negatives, and misclassified 96 negatives (false positives) and 27 positives (false negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MLP model with 10 hidden layers with 25 neurons each\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(25,25,25,25,25,25,25,25,25,25), max_iter=2000, random_state=42) # max_iter default is 200\n",
    "\n",
    "# Fit the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.982\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.54      0.61       226\n",
      "           1       0.99      0.99      0.99      8550\n",
      "\n",
      "    accuracy                           0.98      8776\n",
      "   macro avg       0.85      0.77      0.80      8776\n",
      "weighted avg       0.98      0.98      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 123  103]\n",
      " [  52 8498]]\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics for MLP model with 10 hidden layers with 25 neurons each\n",
    "\n",
    "# Accuracy rounded to 3 decimal places\n",
    "print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 3))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## An accuracy score of 0.985 indicates that the model classifies data very well\n",
    "## The model correctly classified 128 positives and 8520 negatives, and misclassified 98 negatives (false positives) and 30 positives (false negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MLP model with 10 hidden layers with 100 neurons each\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100,100,100,100,100), max_iter=20000, random_state=42) # max_iter default is 200\n",
    "\n",
    "# Fit the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.60      0.68       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.89      0.80      0.84      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 136   90]\n",
      " [  37 8513]]\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics for MLP model with 10 hidden layers with 100 neurons each\n",
    "\n",
    "# Accuracy rounded to 3 decimal places\n",
    "print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 3))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## An accuracy score of 0.986 indicates that the model classifies data very well\n",
    "## The model correctly classified 130 positives and 8523 negatives, and misclassified 96 negatives (false positives) and 27 positives (false negatives)\n",
    "## 10 hidden layers with 100 neurons each is the best model of these three, so I will use this for the rest of the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For alpha =  0.0001\n",
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.60      0.68       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.89      0.80      0.84      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 136   90]\n",
      " [  37 8513]]\n",
      "\n",
      "For alpha =  0.001\n",
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n",
      "\n",
      "For alpha =  0.01\n",
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.53      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.94      0.76      0.83      8776\n",
      "weighted avg       0.99      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 119  107]\n",
      " [  15 8535]]\n"
     ]
    }
   ],
   "source": [
    "# Find the optimum alpha value for MLP model with 100 hidden layers with 100 neurons each\n",
    "\n",
    "alpha = [0.0001, 0.001, 0.01]\n",
    "\n",
    "# Loop through different alpha values to find the best alpha value\n",
    "for a in alpha:\n",
    "    # Run the model\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100,100,100,100,100), alpha=a, max_iter=100000, random_state=42) # max_iter default is 200\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    # Performance metrics\n",
    "    print(\"\\nFor alpha = \", a)\n",
    "    print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 3))\n",
    "    print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## alpha = 0.001 performs the best overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question 2: Analyse impact of different activation function with adam solver on the model**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation function with adam solver\n",
    "relu = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100,100,100,100,100), activation='relu', solver='adam', alpha=0.001, max_iter=20000, random_state=42)\n",
    "\n",
    "# Train the model on our data\n",
    "relu.fit(X_train, y_train)\n",
    "\n",
    "y_pred = relu.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 3))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## All four activation functions perform similarly well in terms of accuracy but relu, tahn, and identity perform slightly better than sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid activation function with adam solver\n",
    "sigmoid = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100,100,100,100,100), activation='logistic', solver='adam', alpha=0.001, max_iter=20000, random_state=42)\n",
    "\n",
    "# Train the model on our data\n",
    "relu.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = relu.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(\"Accuracy: \", round(accuracy_score(y_test, y_pred1), 3))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## All four activation functions perform similarly well in terms of accuracy but relu, tahn, and identity perform slightly better than sigmoid \n",
    "## Sigmoid correctly classifies more true positives (and less false positives) than the other activation functions, but it also misclassifies more false negatives than the other activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n"
     ]
    }
   ],
   "source": [
    "#Tanh activation function with adam solver\n",
    "tanh = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100,100,100,100,100), activation='tanh', solver='adam', alpha=0.001, max_iter=20000, random_state=42)\n",
    "\n",
    "# Train the model on our data\n",
    "tanh.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = relu.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(\"Accuracy: \", round(accuracy_score(y_test, y_pred2), 3))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## All four activation functions perform similarly well in terms of accuracy but relu, tahn, and identity perform slightly better than sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n"
     ]
    }
   ],
   "source": [
    "# Identity activation function with adam solver\n",
    "identity = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100,100,100,100,100), activation='identity', solver='adam', max_iter=20000, random_state=42)\n",
    "\n",
    "# Train the model on our data\n",
    "identity.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = relu.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(\"Accuracy: \", round(accuracy_score(y_test, y_pred3), 3))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For learning_rate =  constant\n",
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n",
      "\n",
      "For learning_rate =  invscaling\n",
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n",
      "\n",
      "For learning_rate =  adaptive\n",
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n"
     ]
    }
   ],
   "source": [
    "# Find the optimum value for learning_rate\n",
    "\n",
    "learning_rate = ['constant', 'invscaling', 'adaptive']\n",
    "\n",
    "# Loop through different alpha values to find the best alpha value\n",
    "for l in learning_rate:\n",
    "    # Run the model\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100,100,100,100,100), activation='relu', solver='adam', alpha=0.001, learning_rate=l, max_iter=100000, random_state=42) # max_iter default is 200\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    # Performance metrics\n",
    "    print(\"\\nFor learning_rate = \", l)\n",
    "    print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 3))\n",
    "    print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## In this case, learning_rate does not affect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For learning_rate_init =  0.0001\n",
      "Accuracy:  0.982\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.65       226\n",
      "           1       0.99      0.99      0.99      8550\n",
      "\n",
      "    accuracy                           0.98      8776\n",
      "   macro avg       0.83      0.81      0.82      8776\n",
      "weighted avg       0.98      0.98      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 141   85]\n",
      " [  70 8480]]\n",
      "\n",
      "For learning_rate_init =  0.001\n",
      "Accuracy:  0.986\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.93      0.77      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 122  104]\n",
      " [  19 8531]]\n",
      "\n",
      "For learning_rate_init =  0.01\n",
      "Accuracy:  0.982\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.41      0.55       226\n",
      "           1       0.98      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.98      8776\n",
      "   macro avg       0.90      0.70      0.77      8776\n",
      "weighted avg       0.98      0.98      0.98      8776\n",
      "\n",
      "Confusion matrix: \n",
      " [[  93  133]\n",
      " [  22 8528]]\n"
     ]
    }
   ],
   "source": [
    "# Find the optimum value learning_rate_init\n",
    "\n",
    "learning_rate_init = [0.0001, 0.001, 0.01] #default is 0.001\n",
    "\n",
    "# Loop through different alpha values to find the best alpha value\n",
    "for l in learning_rate_init:\n",
    "    # Run the model\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100,100,100,100,100), activation='relu', solver='adam', alpha=0.001, learning_rate_init=l, max_iter=100000, random_state=42) # max_iter default is 200\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    # Performance metrics\n",
    "    print(\"\\nFor learning_rate_init = \", l)\n",
    "    print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 3))\n",
    "    print(\"Classification report: \\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "## learning_rate_init = 0.001 performs the best overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question 3:  Explain your findings and report the best performance**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four activation functions perform similarly well in terms of accuracy but relu, tahn, and identity perform slightly better (accuracy = 0.987) than sigmoid (accuracy = 0.986).\n",
    "Tuning the learning rate did not have affect the model's performance, however adjusting the initial learning rate used by the model to 0.001 gave the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
